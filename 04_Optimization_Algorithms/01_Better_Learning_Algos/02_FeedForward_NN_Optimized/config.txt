# Good Configuration for Each Algorithm (FFNN):
 -> GD: Epochs: 10,000 Learning_rate: 0.5
 -> MGD: Epochs: 1,000 Learning_rate: 0.5 Gamma: 0.9 
 -> NAGD: Epochs: 1,000 Learning_rate: 0.5 Gamma: 0.9
 -> SGD: Epochs: 1,000 Learning_rate: 0.01
 -> Minibatch: Epochs: 2,000 Learning_rate: 0.55 Mini-Batch Size: 1024 
 -> Adagrad: Epochs: 500 Learning_rate: 1.0
 -> RMSProp: Epochs: 2000 Learning_rate: 0.01 beta: 0.9
 -> Adam: Epochs: Epochs: 200 Learning_rate: 0.1 beta: 0.9